# ğŸ† Paid or Cloud API Models

## 1ï¸âƒ£ Groq AI
- ğŸš€ We can use models like **LLaMA, DeepSeek, Gemma, and Mistral** for free with some limits.
- âš¡ API response is **very fast**.
- ğŸ™ï¸ OpenAI **Whisper models** are also hosted here.

## 2ï¸âƒ£ OpenAI
- ğŸ¤– A model that **needs no introduction**â€”our **first priority**.
- ğŸ’° **Costly** option.

## 3ï¸âƒ£ Anthropic
- ğŸ† A **good competitor** to OpenAI, our **second priority**.
- ğŸ’² **Cheaper** than OpenAI models.

## 4ï¸âƒ£ Cohere
- ğŸ§ª We can **experiment** and use this if **price & performance** meet our needs.
- ğŸ’° Price is **almost the same** as Anthropic.

## 5ï¸âƒ£ Gemma
- ğŸ’² **Much cheaper** than most models mentioned above.
- ğŸ” Other **Google's closed models** are available here.

## 6ï¸âƒ£ Mistral AI
- ğŸ”’ Other **closed Mistral models** are available here.
- ğŸ’² **Cheaper** than Anthropic and Cohere.

---

# ğŸ’» Locally Runnable Models

## 1ï¸âƒ£ Ollama
- ğŸ  We can run **all open-source models locally** (**GPU is preferred**).
- ğŸ› ï¸ Supports **custom models** (GGUF or safetensors format).
- ğŸ–¥ï¸ We can add **UI as a plugin** too.
- ğŸ”— Once running locally, we can **interact via Python/JS SDKs**.
- ğŸï¸ Supports both **CPU & GPU**.

## 2ï¸âƒ£ Nomic GPT4ALL
- ğŸ¢ Similar to Ollama but offers **enterprise support**.

## 3ï¸âƒ£ LM Studio
- ğŸ—ï¸ Similar to Ollama but **supports ARM chipsets**.
- ğŸ“¡ Has **OpenAI-like API specifications** for all models in its catalog.

## 4ï¸âƒ£ Cortex
- âš™ï¸ Uses **LLaMA.cpp** as the backend inference API **(for now)**.
- ğŸ”® Future plans to use **ONNX as the inference engine**.
- ğŸ’¬ **Jan** is a ChatGPT-like UI built on top of Cortex.

## 5ï¸âƒ£ LLaMA.cpp ([GitHub](https://github.com/ggerganov/llama.cpp))
- ğŸš€ **Plain C & C++ engine** for **state-of-the-art performance**.
- ğŸï¸ **Reduced memory usage** & supports **ARM chips**.
- ğŸ”¥ Supports **1.5-bit quantization** for faster inference.
- ğŸï¸ **Hybrid CPU + GPU inference**.
- ğŸ› ï¸ SDKs available in **Python, JS, Java, C#, Flutter, Go, etc.**
- ğŸ–¥ï¸ Supports **UI plugins** (even for AI code editing!).

## 6ï¸âƒ£ Local AI
- ğŸ”§ Supports **LLaMA.cpp**.
- ğŸ› ï¸ Features: **TTS, transcription, image generation, embeddings, store, re-ranker, etc.**

## 7ï¸âƒ£ Llamafile ([GitHub](https://github.com/Mozilla-Ocho/llamafile))
- ğŸ“¦ **Higher-level abstraction** over **LLaMA.cpp** for easier deployment.
- âš¡ **Same performance** as LLaMA.cpp but requires **no library installation**.
- ğŸ“¡ **Supports OpenAI API spec**.
- âš ï¸ **Still a bit unstable** at the time of testingâ€”proper testing is recommended before production use!
