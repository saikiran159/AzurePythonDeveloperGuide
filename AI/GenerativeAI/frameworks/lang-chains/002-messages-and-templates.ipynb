{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d79a62-c5ef-460a-8f75-e54a5819748b",
   "metadata": {},
   "source": [
    "## What We've Learned So Far ‚úÖ\n",
    "\n",
    "1. **Why do we need function calling?** ü§î\n",
    "2. **How to create and invoke function calls** üõ†Ô∏è\n",
    "3. **How to structure LLM output** üìú\n",
    "4. **Caching LLM output for speed and cost efficiency** üí∞‚ö°\n",
    "5. **How to parameterize model parameters using configurables** üîß\n",
    "\n",
    "If you haven't learnt these things, please go through this notebook again [001-chat-models.ipynb](001-chat-models.ipynb)\n",
    "\n",
    "\n",
    "---\n",
    "# üöÄ Exploring New Concepts in LLMs\n",
    "\n",
    "Now, let's focus on some new things here.  \n",
    "\n",
    "## 1Ô∏è‚É£ Structuring Input for LLMs  \n",
    "\n",
    "- We already know how to structure **LLM model responses**, but how should we **structure the input** sent to an LLM?  \n",
    "- What are the **benefits** of structuring inputs properly?  \n",
    "- We'll explore these questions and understand the impact of well-structured inputs.  \n",
    "\n",
    "## 2Ô∏è‚É£ Differentiating Messages in LLM Interactions  \n",
    "\n",
    "- When we send a message to an LLM, it **replies back** with a new message.  \n",
    "- We provide **instructions** to the LLM as messages.  \n",
    "- However, we are using the same term, **\"messages,\"** for everything, which can cause confusion. ü§î  \n",
    "- To solve this, **LangChain** provides a better way to **differentiate these messages**‚Äîwe'll look into how it works.  \n",
    "\n",
    "## 3Ô∏è‚É£ Understanding Context Window Limitations  \n",
    "\n",
    "- LLM models have something called a **context window** üìè, meaning they can handle only a **specific number of tokens** (aka words) per request.  \n",
    "- This limitation is a **challenge** because we want the LLM to **retain more context** for better responses.  \n",
    "- Fortunately, **LangChain offers a solution** to this problem‚Äîwe'll explore that too!  \n",
    "\n",
    "Stay tuned as we dive deeper into these concepts! üîç‚ú®  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0e0f9-11d1-4bbd-9d7d-b96c7919bf3c",
   "metadata": {},
   "source": [
    "### Let's Start with Message Types\n",
    "\n",
    "1. **SystemMessage**: We use this to instruct LLM on what it has to do\n",
    "2. **HumanMessage**: We use this to communicate to LLM (input to LLM)\n",
    "3. **AIMessage**: llm message will be of this type\n",
    "4. **ToolMessage**: We will take out this bit later, but this message type is used to store `tool output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00accb4-fee2-47f5-888d-8259df9d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage,ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43ed6c1-12af-4c50-905d-933b6a8850d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_input = HumanMessage(content=\"Hello GPT. i would like to know weather details in my location\", # content is used to store the message itself\n",
    "                          id=\"123\", # id must be unique for all messages. we identify a unique message only with this unique id\n",
    "                          name=\"alex\", # a top-level filter on id. So when we want to filter messages, we start with name so that messages will be from specific entity and id to get specific id\n",
    "                          custom_metadata={ # this is custom metadata\n",
    "                              \"phone_no\":\"+1867389126\",\n",
    "                              \"location\":\"santa clara\"\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20e1c02-1cb1-4134-815c-83119184dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello GPT. i would like to know weather details in my location'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_input.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e126e6-662f-4488-ba9a-6a0b879755a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content, name and id are common arguments are there for system_message too\n",
    "system_message = SystemMessage(content=\"You are dolphin a helpful ai assistant\",id=\"321\",name=\"persona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c254d75d-8685-49b4-9869-0df34f6a9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMessage and ToolMessage have more attributes to play with\n",
    "\n",
    "ai_message = AIMessage(content=\"hello! i am dolphin your ai assistant. what do you want to know?\",\n",
    "                      name=\"model\",\n",
    "                      id=\"604880nv4\",\n",
    "                      tool_calls=[{ # additional param to store tool_calls\n",
    "                          \"name\":\"multiplier\", # example. i didnt define this tool\n",
    "                          \"args\":{\"a\":40,\"b\":50},\n",
    "                          \"id\":\"ADTG3793Y9\", # this id is a combination of toolname and it's uniqueness. I mean if same tool is invoked twice. this value will be different\n",
    "                          \"type\":\"tool_call\"\n",
    "                      }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9665b54c-d535-4cb2-a2f5-7c758833c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiplier',\n",
       "  'args': {'a': 40, 'b': 50},\n",
       "  'id': 'ADTG3793Y9',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7da17f1f-b2ee-483a-8fc1-d49cae24fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_message = ToolMessage(content=\"2000\",\n",
    "                           name=\"multiplier_tool_result\",\n",
    "                           id=\"fvi3r792bc22\", # this is unique id for the message\n",
    "                          tool_call_id=\"ADTG3793Y9\",# this is an example, id for tool mentioned above and this will be same\n",
    "                           artifact={\"log\":\"result is an even number\"}, # sometimes tool might return some metadat which are required but not used by model. these can be stored here\n",
    "                           status=\"success\", # this is a status Literal message with either success or error in it\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a31f38f-8e2b-4a08-8951-fb9088e34c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='2000', name='multiplier_tool_result', id='fvi3r792bc22', tool_call_id='ADTG3793Y9', artifact={'log': 'result is an even number'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "44d7fd36-eed8-47ff-bbe6-bb5da5430835",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can invoke our LLM with these message types\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"deepseek-r1-distill-llama-70b\",model_provider=\"groq\",max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bea7ea74-948e-47e9-9a25-9412978c04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.invoke([system_message,human_input]) # this is how you can invoke your llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eabd0d3a-eed7-4a1d-9b0f-3286b02e28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = system_message + human_input # yes we can add messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23adfd25-71d7-4294-b1ee-21c921f06e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_input) # This is our next topic. we will explore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27ce0a26-14c5-4d91-8fb5-d1ec5d484aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are dolphin a helpful ai assistant', additional_kwargs={}, response_metadata={}, name='persona', id='321'),\n",
       " HumanMessage(content='Hello GPT. i would like to know weather details in my location', additional_kwargs={}, response_metadata={}, name='alex', id='123', custom_metadata={'phone_no': '+1867389126', 'location': 'santa clara'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.messages # you can see both messages in a list here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaeabb1b-fd71-43f8-baba-2611025f73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## there is one more think called Chunk, this chunk exist in all above message types\n",
    "from langchain_core.messages import HumanMessageChunk,AIMessageChunk\n",
    "\n",
    "hm1 = HumanMessageChunk(content=\"I am using a desktop computer for this tutorial.\") # same arguments as in normal HumanMessage\n",
    "hm2 = HumanMessageChunk(content=\"I am noting down all the points i have discussed and about to discuss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "91fd3dd7-d8b8-4f8c-b54e-b9759e58e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessageChunk(content='I am using a desktop computer for this tutorial.I am noting down all the points i have discussed and about to discuss.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm1 + hm2 # Add two chunks, you will get a new chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "966ad92c-c1ba-4439-8f32-5f4de7531ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[HumanMessage(content='Hello GPT. i would like to know weather details in my location', additional_kwargs={}, response_metadata={}, name='alex', id='123', custom_metadata={'phone_no': '+1867389126', 'location': 'santa clara'}), HumanMessage(content='Hello GPT. i would like to know weather details in my location', additional_kwargs={}, response_metadata={}, name='alex', id='123', custom_metadata={'phone_no': '+1867389126', 'location': 'santa clara'})])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_input + human_input # But add two same or different messages, you will get a chatprompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4508666-1474-495b-8880-5b2e2ef03e9d",
   "metadata": {},
   "source": [
    "## Now Let's Structure our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "74f9ff32-af6a-432f-9c00-ae7a141deb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 7, 'total_tokens': 51, 'completion_time': 0.16, 'prompt_time': 0.003494816, 'queue_time': 0.023829819, 'total_time': 0.163494816}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_af2982c95d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2aabfaec-7d1e-466c-8588-eb34baf5ff0a-0', usage_metadata={'input_tokens': 7, 'output_tokens': 44, 'total_tokens': 51})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are many ways we can prompt the model\n",
    "\n",
    "# 1. simple prompt (string)\n",
    "llm.invoke(\"what is your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "747e0792-629a-4ffb-b58d-7ee5d1f72dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so the user wants a joke about a fianc√©, and they want the response to be short, around 50 words. Let me brainstorm some ideas. Maybe something playful about marriage or engagement. I should make sure it\\'s lighthearted and not offensive. Let\\'s think about common fianc√© stereotypes or situations. Oh, maybe something about love and commitment. How about a play on words? Maybe something like, \"Why did the fianc√© bring a ladder to the wedding? Because they wanted to take their relationship to new heights!\" Hmm, that\\'s a bit cheesy but works. Alternatively, I could go with something about the ring or planning the wedding. Wait, the user already provided an example, so I can use that as a model. Their example is about love being blind and marriage opening the eyes, which is a classic twist. Maybe I can think of another angle. Perhaps something about the fianc√© forgetting something, like \"Why did the fianc√© forget his keys? Because he was too excited to unlock a new life together!\" Not sure if that\\'s as good. Maybe stick with the original idea but tweak it. Alternatively, think about the fianc√© being nervous or excited. Maybe \"Why did the fianc√© bring a magnet? To attract a lifelong commitment!\" No, that\\'s not as smooth. Maybe \"Why did the fianc√© get a dog? For practice before the kids!\" That\\'s more about parenting. Hmm, maybe \"Why did the fianc√© learn sign language? To communicate love in every way!\" That\\'s sweet but not a joke. Okay, perhaps I\\'ll go with the user\\'s example as it\\'s effective and fits the 50-word limit. So, the fianc√© joke would be: \"Why don‚Äôt fianc√©s make good secret agents? Because they can‚Äôt keep their engagement under wraps!\" That\\'s another option. I think the user\\'s example is better, though. So, I\\'ll go with that.\\n</think>\\n\\n\"Why don‚Äôt fianc√©s make good secret agents? Because they can‚Äôt keep their engagement under wraps!\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 423, 'prompt_tokens': 30, 'total_tokens': 453, 'completion_time': 1.538181818, 'prompt_time': 0.004254998, 'queue_time': 0.022616277, 'total_time': 1.542436816}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_af2982c95d', 'finish_reason': 'stop', 'logprobs': None}, id='run-9e785bb2-5e0e-4e49-96c1-f504c943fff1-0', usage_metadata={'input_tokens': 30, 'output_tokens': 423, 'total_tokens': 453})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. simple prompt (string)\n",
    "topic = \"fiance\"\n",
    "llm.invoke(f\"\"\"\n",
    "Please get me only short responses (50 words) for the questions i am going to ask.\n",
    "\n",
    "tell me a joke on {topic}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be9dfd33-5df4-49a0-a2fd-f0bf635a8717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nAlright, so the user is asking for a joke about Iron Man from the Marvel universe. They also specified that the response should be in 10 words. Hmm, I need to come up with something that\\'s both funny and concise. \\n\\nFirst, I should think about Iron Man\\'s key traits. He\\'s a billionaire, a genius inventor, and of course, he wears that iconic red and gold suit. Maybe I can play on words related to his suit or his personality. \\n\\nI remember that Iron Man\\'s suit is powered by an arc reactor, but that might be a bit too technical for a joke. Maybe something about his wealth? Like, why did Iron Man bring a ladder to the party? Because he heard the drinks were on the house! That plays on the phrase \"on the house,\" meaning free drinks, but also references the height, which a ladder would help with. Plus, it\\'s within the 10-word limit. \\n\\nWait, let me check the word count. \"Why did Iron Man bring a ladder to the party? Because he heard the drinks were on the house!\" That\\'s exactly 15 words. Oops, too long. I need to shorten it. \\n\\nHow about: \"Why did Iron Man bring a ladder? Drinks were on the house!\" That\\'s 10 words. Perfect! It\\'s concise, ties into his billionaire status implying the drinks are free, and the ladder adds the pun. \\n\\nI think that works. It\\'s a classic setup and punchline structure, easy to understand, and fits within the word limit. Plus, it\\'s light-hearted and suitable for most audiences. \\n\\nI should also consider if there are other angles, but given the word constraint, this seems the most fitting. Maybe another approach, but I can\\'t think of a better one right now. So, this should be the response.\\n</think>\\n\\nWhy did Iron Man bring a ladder? Drinks were on the house!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 394, 'prompt_tokens': 24, 'total_tokens': 418, 'completion_time': 1.432727273, 'prompt_time': 0.004000973, 'queue_time': 0.020666961999999997, 'total_time': 1.436728246}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_af2982c95d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6f5d9aa7-cf07-416f-8cdd-c065f27b61b5-0', usage_metadata={'input_tokens': 24, 'output_tokens': 394, 'total_tokens': 418})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. chat prompt (json openai format)\n",
    "llm.invoke([\n",
    "    {\"role\":\"system\",\"content\":\"please make sure to return your response in 10 words\"},\n",
    "    {\"role\":\"user\",\"content\":\"tell me a joke about iron man in marvel\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9c1c96a4-c307-460b-a5cb-6f8cddf3de6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so the user wants a joke about Iron Man from Marvel. Let me think... Iron Man\\'s suit is a big part of his character, so maybe I can make a pun around that. Why did Iron Man\\'s suit go to therapy? Hmm, because it had a lot of \"metal\" issues. That plays on \"metal\" as in the suit\\'s material and \"mental\" issues, which is a common pun. It\\'s simple and should bring a smile. I think that works.\\n</think>\\n\\nWhy did Iron Man\\'s suit go to therapy? It had too many *metal* issues.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 24, 'total_tokens': 149, 'completion_time': 0.454545455, 'prompt_time': 0.0056447, 'queue_time': 0.096018251, 'total_time': 0.460190155}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_95405939f8', 'finish_reason': 'stop', 'logprobs': None}, id='run-39421864-af81-4bc3-a89a-57caf2d3d353-0', usage_metadata={'input_tokens': 24, 'output_tokens': 125, 'total_tokens': 149})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. chat prompt (tuples)\n",
    "llm.invoke([\n",
    "    (\"system\",\"please make sure to return your response in 10 words\"),\n",
    "    (\"user\",\"tell me a joke about iron man in marvel\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b4269088-dc7e-445b-8862-49c9e014d0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to come up with a joke about Iron Man from Marvel. Let me think about Iron Man\\'s characteristics. He\\'s a superhero with a suit made of iron, right? Oh, wait, no, it\\'s not actually iron, it\\'s more advanced metals and technology. But \"iron\" is in his name, so maybe that\\'s a pun opportunity.\\n\\nI remember that in the Marvel movies, Tony Stark, who is Iron Man, is a bit of a jokester himself. Maybe I can play on that. Let me think about things related to iron. Iron is a metal, and in everyday language, people might say things like \"ironing out the wrinkles\" or \"ironing clothes.\" That could be a good angle.\\n\\nSo, if Iron Man were to make a joke, maybe it would involve something he does. Since he\\'s always suited up and saving the day, perhaps he could joke about something mundane. Maybe he could say something about not needing to iron his suit because it\\'s always perfect. Or maybe something about how he doesn\\'t have to iron clothes because he\\'s too busy being a superhero.\\n\\nWait, but the user wants the joke to be in 10 words. So I need to make it concise. Let me try to structure it as a question and answer. Maybe something like, \"Why doesn\\'t Iron Man need an iron?\" And the punchline could be, \"Because he\\'s already in a suit!\" Hmm, that\\'s a bit short. Let me count the words: \"Because he\\'s already in a suit!\" That\\'s 6 words. Maybe I can add a bit more to make it 10. Or perhaps rephrase it.\\n\\nAlternatively, \"Why did Iron Man refuse to iron his clothes?\" \"Because he\\'s already pressed into service!\" That\\'s a bit longer but still concise. Let me count: \"Because he\\'s already pressed into service!\" That\\'s 7 words. Maybe I can make it 10 by adding a bit more, but I don\\'t want to make it too long.\\n\\nWait, another angle: Iron Man\\'s suit is made of metal, so maybe something about rust? But rust jokes might be a bit forced. Or maybe something about his arc reactor. But that might be too technical.\\n\\nLet me go back to the ironing idea. \"Why doesn\\'t Iron Man iron his clothes?\" \"Because he\\'s already suited up!\" That\\'s 8 words. Maybe add \"in iron\" at the end to make it 10: \"Because he\\'s already suited up in iron!\" Hmm, but that might not flow well.\\n\\nWait, maybe a play on \"ironing out the kinks.\" So, \"Why doesn\\'t Iron Man iron his clothes?\" \"Because he\\'s too busy ironing out the kinks in his suit!\" That\\'s 12 words. Too long.\\n\\nAlternatively, \"Why doesn\\'t Iron Man need an iron?\" \"Because his suit is always pressed!\" That\\'s 9 words. Close enough, but maybe not as funny.\\n\\nWait, let me think differently. Maybe a pun on \"iron\" as in the metal and \"ironing\" as the action. So, \"Why did Iron Man bring an iron to the fight?\" \"Because he wanted to press his advantage!\" That\\'s 11 words. A bit too long, but the pun is there.\\n\\nAlternatively, \"Iron Man doesn\\'t iron clothes; he presses charges!\" That\\'s 8 words, but it\\'s a bit of a stretch.\\n\\nWait, maybe \"Why doesn\\'t Iron Man do laundry?\" \"Because he\\'s always ironing out problems!\" That\\'s 9 words. Maybe that works.\\n\\nAlternatively, \"Why did Iron Man skip ironing?\" \"Because he\\'s already metal!\" That\\'s 8 words. Short and sweet.\\n\\nHmm, but the user asked for 10 words. Let me see if I can adjust it. \"Because he\\'s already made of metal!\" That\\'s 6 words. Maybe \"Because he\\'s already made of iron!\" 6 words.\\n\\nWait, the initial attempt was \"Because he\\'s already in a suit!\" That\\'s 6 words. Maybe I can add a bit more. \"Because he\\'s already in a metal suit!\" That\\'s 7 words. Still short.\\n\\nWait, perhaps the joke can be a play on words without needing to be exactly 10 words. But the user specified 10 words, so I should aim for that.\\n\\nLet me try: \"Why doesn\\'t Iron Man ever iron his clothes?\" \"Because he\\'s already suited up in iron!\" That\\'s 9 words. Close. Maybe add \"every day\" at the end: \"Because he\\'s already suited up in iron every day!\" Now it\\'s 10 words.\\n\\nBut does that make sense? Iron Man doesn\\'t iron because he\\'s already wearing his suit every day. That could work.\\n\\nAlternatively, \"Why doesn\\'t Iron Man need an iron?\" \"Because his suit is always ironed!\" That\\'s 9 words. Maybe add \"and ready\" to make it 10: \"Because his suit is always ironed and ready!\" That\\'s 8 words. Hmm.\\n\\nWait, maybe a different approach. \"What did Iron Man say when his clothes were wrinkled?\" \"I don\\'t have time for that, I\\'m busy saving the world!\" That\\'s longer than 10 words.\\n\\nAlternatively, \"Iron Man\\'s secret: no iron, just suit up!\" That\\'s 7 words.\\n\\nI think the best approach is to go with the ironing pun. So, \"Why doesn\\'t Iron Man ever iron his clothes?\" \"Because he\\'s already suited up in iron!\" That\\'s 9 words. Maybe add \"daily\" to make it 10: \"Because he\\'s already suited up in iron daily!\" Now it\\'s 8 words. Hmm.\\n\\nWait, maybe rephrase the question to be shorter. \"Why no ironing for Iron Man?\" \"Because he\\'s already in a suit!\" That\\'s 8 words. Or \"Because he\\'s already suited up!\" 5 words.\\n\\nI think the key is to make it concise and within 10 words. So, \"Why doesn\\'t Iron Man iron?\" \"Because he\\'s already in a suit!\" That\\'s 9 words. Close enough.\\n\\nAlternatively, \"Iron Man: no iron needed, suit\\'s always pressed!\" That\\'s 8 words.\\n\\nI think I\\'ll settle on: \"Why doesn\\'t Iron Man need an iron? Because he\\'s already in a suit!\" That\\'s 12 words. Oops, too long.\\n\\nWait, the initial response I gave was: \"Because he\\'s already in a suit!\" That\\'s 6 words. Maybe the user wants exactly 10, so perhaps: \"Because he\\'s already wearing a metal suit!\" That\\'s 7 words.\\n\\nAlternatively, \"Because he\\'s already suited up in iron!\" 6 words.\\n\\nI think I\\'ll go with: \"Because he\\'s already in a suit made of iron!\" That\\'s 8 words. Maybe the user doesn\\'t mind a few extra words for clarity.\\n\\nWait, but the user specified 10 words. So, perhaps: \"Because he\\'s already wearing a suit made of iron daily!\" That\\'s 9 words.\\n\\nI think the best way is to keep it concise and within the word limit, even if it\\'s a bit forced. So, \"Because he\\'s already suited up in iron!\" 6 words. Maybe the user will accept it as is.\\n\\nAlternatively, maybe the user doesn\\'t mind a slightly longer response. So, I\\'ll go with the initial version: \"Because he\\'s already in a suit!\" That\\'s 6 words, but it\\'s clear and funny.\\n</think>\\n\\nBecause he\\'s already in a suit!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1579, 'prompt_tokens': 24, 'total_tokens': 1603, 'completion_time': 5.741818182, 'prompt_time': 0.004116321, 'queue_time': 0.023721523, 'total_time': 5.745934503}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_af2982c95d', 'finish_reason': 'stop', 'logprobs': None}, id='run-67d1c23b-1a10-4365-b3bc-69ce12d1dd24-0', usage_metadata={'input_tokens': 24, 'output_tokens': 1579, 'total_tokens': 1603})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. chat prompt (MessageTypes)\n",
    "llm.invoke([\n",
    "    SystemMessage(content=\"please make sure to return your response in 10 words\"),\n",
    "    HumanMessage(content=\"tell me a joke about iron man in marvel\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fcb19-e863-4c1c-b6ee-4f6df007b9de",
   "metadata": {},
   "source": [
    "### Now Let's parameterize some of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a7be8b46-21e3-4af5-8fb0-b79a7eb60e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Hi! My name is alex')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "# For simpler string prompts\n",
    "prompt = PromptTemplate.from_template(\"Hi! My name is {person}\")\n",
    "prompt.invoke({\"person\":\"alex\"}) # we can see even prompt can be invokable not only model or tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e41ca2-55d8-4847-b1cb-800a563c0ab9",
   "metadata": {},
   "source": [
    "What if we want to add many **few-shot examples** to the **string prompt**. we dont want to create one big repeated string\n",
    "\n",
    "1. we need a format for each example. so first, we need to create template for this.\n",
    "2. now we need to have a list of all few-shot examples\n",
    "3. with above two points, langchain will create a bigstring for us. now we want to add prefix and suffix for that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "662237bc-62cd-4caa-8d92-42cb2986d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate,PromptTemplate\n",
    "\n",
    "few_shot_template = PromptTemplate.from_template(\"user: {user}\\nai: {ai}\")\n",
    "examples = [\n",
    "    {\"user\":\"Add 5 and 2\",\"ai\":\"Addition: 7\"},\n",
    "    {\"user\":\"Multiply 10 and 5\",\"ai\":\"Multiplication: 50\"}\n",
    "]\n",
    "final_template = FewShotPromptTemplate(example_prompt=few_shot_template,\n",
    "                                       examples=examples,\n",
    "                                       prefix=\"Observe Below example conversations and answer accordingly\",\n",
    "                                       suffix=\"user: {user}\\n ai: \",\n",
    "                                       input_variables=[\"user\"]\n",
    "                                      )\n",
    "\n",
    "template_value = final_template.invoke({\"user\":\"Add 10 and 7\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7a784030-14d6-4158-bf2f-da48c61a691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observe Below example conversations and answer accordingly\n",
      "\n",
      "user: Add 5 and 2\n",
      "ai: Addition: 7\n",
      "\n",
      "user: Multiply 10 and 5\n",
      "ai: Multiplication: 50\n",
      "\n",
      "user: Add 10 and 7\n",
      " ai: \n"
     ]
    }
   ],
   "source": [
    "print(template_value.text) # You can see below prefix, examples and suffix are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c2b61bba-9705-4ec6-87ef-8d118e68f6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nI need to analyze the user\\'s request based on the provided examples.\\n\\nIn the examples, the user asks for a calculation, and the AI responds with the operation and the result.\\n\\nThe user\\'s current query is \"Add 10 and 7\".\\n\\nFollowing the pattern, I should respond with \"Addition: 17\".\\n</think>\\n\\nAddition: 17', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 57, 'total_tokens': 132, 'completion_time': 0.272727273, 'prompt_time': 0.010422477, 'queue_time': 0.259157912, 'total_time': 0.28314975}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_af2982c95d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0967acd3-186d-4ee3-92b4-c29f21106d13-0', usage_metadata={'input_tokens': 57, 'output_tokens': 75, 'total_tokens': 132})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(template_value) # invoked with llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fce4b0b6-752c-4eca-9beb-f6622aae6387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to figure out how to respond to the user\\'s instruction based on the examples provided. Let\\'s see, the user has given a pattern where they ask for a mathematical operation between two numbers, and the AI responds with the operation\\'s name followed by the result.\\n\\nLooking at the first example: the user says \"Add 5 and 2\", and the AI responds with \"Addition: 7\". That makes sense because 5 plus 2 equals 7. \\n\\nThen, the second example is \"Multiply 10 and 5\", and the AI says \"Multiplication: 50\". Again, that\\'s correct because 10 multiplied by 5 is 50.\\n\\nNow, the third example is \"Add 10 and 7\", and the AI\\'s response is cut off. Following the pattern, the AI should respond with \"Addition: 17\" because 10 plus 7 is 17.\\n\\nSo, the user is probably testing if I can follow the same pattern. They might be checking if I can recognize the operation and compute the result correctly. Maybe they\\'re evaluating my ability to handle basic arithmetic operations based on the given examples.\\n\\nI should make sure to respond in the same format: start with the operation\\'s name, followed by a colon and space, then the result. That way, it\\'s consistent with the examples provided.\\n\\nI don\\'t think the user is looking for anything more complicated here. It\\'s a straightforward arithmetic problem, so I shouldn\\'t overcomplicate it. Just apply the correct operation and present it in the specified format.\\n\\nAlso, considering the previous responses, the AI didn\\'t use any markdown or special formatting, just plain text. So I should stick to that as well.\\n\\nI wonder if there might be any trick or twist here, but given the examples, it\\'s probably just a simple addition. The user might be trying to ensure that I can correctly identify the operation from the instruction and perform the calculation accurately.\\n\\nIn summary, my response should be \"Addition: 17\" to match the pattern and correctly solve the problem.\\n</think>\\n\\nAddition: 17', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 434, 'prompt_tokens': 57, 'total_tokens': 491, 'completion_time': 1.578181818, 'prompt_time': 0.005174156, 'queue_time': 0.051388482, 'total_time': 1.583355974}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_95405939f8', 'finish_reason': 'stop', 'logprobs': None}, id='run-d836f897-f782-4cf8-a011-b9fed708fd86-0', usage_metadata={'input_tokens': 57, 'output_tokens': 434, 'total_tokens': 491})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm.invoke(final_template.invoke({\"user\":\"Add 10 and 7\"})) # instead of doing this, we can use LCEL. we will go through this later deeply\n",
    "\n",
    "# LCEL syntax aka chains\n",
    "new_llm = final_template | llm # invoked params first goes through final_template and output which is stringprompt aka normalstring is passed to llm\n",
    "new_llm.invoke({\"user\":\"Add 10 and 7\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "18fb4d7d-b01e-4e09-ae29-39b7b41acbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant in Finance', additional_kwargs={}, response_metadata={}), HumanMessage(content='I need your help in Doing my taxes', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat prompt inputs with tuples we will use ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate([(\"system\",\"You are a helpful assistant in {topic}\"),\n",
    "                             (\"user\",\"I need your help in {task}\")\n",
    "                            ])\n",
    "prompt.invoke({\"topic\":\"Finance\",\"task\":\"Doing my taxes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "83dba747-77c9-4b88-bc71-267ce87d284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant in Lawsuits', additional_kwargs={}, response_metadata={}), HumanMessage(content='I need your help in filing a cases againt racoons. They are invading my garden.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat prompt inputs with openai like request we will use ChatPromptTemplate\n",
    "openai_prompt = ChatPromptTemplate([\n",
    "    {\"role\":\"system\",\"content\":\"You are a helpful assistant in {topic}\"},\n",
    "    {\"role\":\"user\",\"content\":\"I need your help in {task}\"}])\n",
    "\n",
    "openai_prompt.invoke({\"topic\":\"Lawsuits\",\"task\":\"filing a cases againt racoons. They are invading my garden.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6cb18fda-143f-421d-b763-6d6689ef0f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant in financial advising', additional_kwargs={}, response_metadata={}), HumanMessage(content='I need your help in suggesting me whether i can afford jeep wrangler rubicon?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat prompt inputs with langchain messagetypes we will use ChatPromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "openai_prompt = ChatPromptTemplate([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant in {topic}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"I need your help in {task}\")])\n",
    "\n",
    "openai_prompt.invoke({\"topic\":\"financial advising\",\"task\":\"suggesting me whether i can afford jeep wrangler rubicon?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e8dc0f9a-6f19-4c95-9d0b-a205e0257b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt.invoke({\"topic\":\"Finance\",\"task\":\"Doing my taxes\"})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "81a66e22-2a20-4036-90a9-a253670da07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to do my taxes, but I'm not really sure where to start. Let me try to break this down step by step. First, I remember from last year that I needed some forms from my employer. I think it's called a W-2? Yeah, that sounds right. I should probably check my emails or maybe my employer's portal to see if that's available yet. If I can't find it, I guess I'll have to contact HR or my HR department. \\n\\nNext, I think I need to report any other income I had. I did some freelance work over the summer, so I should get a 1099 form for that. Wait, do I get a 1099 every time I freelance, or is it only if I made a certain amount? I think it's if I made over $600, but I'm not entirely sure. Maybe I should look that up to confirm. Also, I sold some stuff online, like on eBay. I'm not sure if that counts as income or just personal stuff. I think if it's personal, it's not taxable, but if I was selling things regularly, maybe it's considered business income. I'm a bit confused about that. \\n\\nI also have a savings account, so I should get a 1099-INT form for the interest earned. I remember getting that last year, and it wasn't much, but still, I should include it. Oh, and my bank might have sent me a 1099-DIV if I had any dividends from investments. I'm not sure if I have any, but I should check.\\n\\nNow, about deductions. I heard that you can deduct certain things to lower your taxable income. I paid some student loan interest this year, so maybe that can be deducted. Also, I think I can deduct the interest on my student loans, but I'm not sure how that works. I should find out what the limit is on that deduction.\\n\\nI moved to a new apartment this year, and I heard that moving expenses might be deductible, especially if it's for a job. I need to see if my move qualifies. I kept some receipts for the moving truck and storage, so maybe I can add those up and deduct them.\\n\\nI also donated some clothes and furniture to charity. I think I can get a deduction for that, but I need to figure out the fair market value. I remember getting a receipt from the charity, so I should keep that. \\n\\nI'm a student, so I might qualify for education credits. I paid some tuition this year, so I should look into the American Opportunity Tax Credit or the Lifetime Learning Credit. I'm not sure which one applies, but I think one of them gives a better deal if I'm pursuing a degree.\\n\\nI have health insurance through the marketplace, so I might be eligible for the premium tax credit. I think that's based on my income, so I need to make sure I have my income figures right when I apply for that.\\n\\nI'm not sure if I should itemize my deductions or just take the standard deduction. I think the standard deduction is a fixed amount, but if my itemized deductions add up to more than that, it's better to itemize. I need to calculate both and see which gives me a better deal.\\n\\nI also need to decide how much I want to get back or if I want to owe a little extra. Last year, I got a big refund, which was nice, but I don't like the idea of loaning the government my money interest-free. Maybe I should adjust my withholdings so that my refund is smaller, and I take more home each month.\\n\\nI'm a bit confused about the tax brackets. I think each portion of my income is taxed at different rates, so I need to figure out which bracket I'm in. That might help me understand how much I'll owe or get back.\\n\\nI have some retirement contributions. I contributed to my IRA this year, so maybe I can deduct that. I'm not sure about the Roth IRA, though, because I think contributions are after-tax, so maybe they aren't deductible. I need to check the rules on that.\\n\\nI also have some medical expenses. I went to the doctor a few times and got some prescriptions. I should add those up and see if they exceed a certain percentage of my income, which I think is 10%. If they do, I might be able to deduct them.\\n\\nI'm planning to file as single, but I wonder if there's a benefit to filing as head of household. I don't have dependents, so I don't think that applies. Maybe single is the right status for me.\\n\\nI think I should gather all my documents first before I start filling anything out. That includes my W-2, 1099s, receipts for deductions, charity receipts, medical bills, and any other relevant documents. I should also make sure I have my Social Security number and the Social Security numbers for any dependents, although I don't have any.\\n\\nI'm considering using tax software like TurboTax or H&R Block. They seem user-friendly, but I'm not sure if I can handle it on my own. Maybe I should look for some guides or tutorials online to help me through the process. If I get stuck, I might have to ask for help or even consult a tax professional, but I'd prefer to do it myself to save money.\\n\\nI should also make a note of the filing deadline. I think it's usually April 15th, but sometimes it's a different date if that's a weekend or a holiday. I don't want to miss the deadline and end up with penalties.\\n\\nLastly, once I'm done, I should review everything to make sure I didn't miss anything. Maybe I can have a friend or family member look it over to catch any mistakes I might have made. I should also e-file so that I get my refund faster and don't have to deal with mailing paper forms.\\n\\nI think that's a rough plan. Now, let me try to outline the steps more clearly and make sure I cover everything I need to do.\\n</think>\\n\\nTo effectively prepare for filing your taxes, follow this organized plan:\\n\\n### 1. **Gather Necessary Documents:**\\n   - **Income Documents:**\\n     - W-2 from your employer.\\n     - 1099 forms for freelance work (if earned over $600).\\n     - 1099-INT for interest from savings accounts.\\n     - 1099-DIV for any dividends from investments.\\n   - **Deduction and Credit Documents:**\\n     - Student loan interest statements.\\n     - Moving expense receipts.\\n     - Charity donation receipts with fair market value.\\n     - Tuition payments for education credits.\\n     - Health insurance premium statements.\\n     - Medical expense receipts.\\n     - Retirement contribution statements (IRA, etc.).\\n\\n### 2. **Determine Filing Status:**\\n   - You will file as **Single** since you have no dependents.\\n\\n### 3. **Calculate Income:**\\n   - Sum all income from W-2, 1099s, freelance work, and any other sources.\\n\\n### 4. **Evaluate Deductions:**\\n   - **Itemized vs. Standard Deduction:**\\n     - Calculate itemized deductions (student loan interest, moving expenses, charitable donations, medical expenses) and compare to the standard deduction.\\n   - **Education Credits:**\\n     - Explore eligibility for the American Opportunity Tax Credit or Lifetime Learning Credit.\\n   - **Health Insurance:**\\n     - Determine eligibility for the Premium Tax Credit.\\n\\n### 5. **Tax Credits and Adjustments:**\\n   - **Retirement Contributions:**\\n     - Deduct contributions to traditional IRA if applicable.\\n   - **Other Credits:**\\n     - Consider the Savers Credit for retirement contributions.\\n\\n### 6. **Choose Filing Method:**\\n   - Use tax software like TurboTax or H&R Block for a user-friendly experience. Consider tutorials or guides if needed.\\n\\n### 7. **Review and Submit:**\\n   - Double-check all information for accuracy.\\n   - E-file to expedite processing and receive your refund faster.\\n\\n### 8. **Post-Filing:**\\n   - **Refund or Payment:**\\n     - Decide on refund method (direct deposit, check) or payment method if owed.\\n   - **Adjust Withholdings:**\\n     - Review W-4 to adjust withholdings for the next year if desired.\\n\\n### 9. **Stay Informed:**\\n   - Note the filing deadline (typically April 15th).\\n   - Keep all documents and a copy of your return for records.\\n\\nBy following these steps, you can efficiently navigate the tax filing process, ensuring you maximize your refund and comply with all tax requirements.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "393b476d-93a4-426c-9ec3-8435bd214e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='you know my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Sorry you haven't told me before. You can say it now,i will remember\", additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is rahul', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"user_inputs\") # you can mention a placeholder and you can invoke in prompt\n",
    "])\n",
    "prompt.invoke({\"user_inputs\":[\n",
    "    (\"user\",\"you know my name?\"),\n",
    "    (\"ai\",\"Sorry you haven't told me before. You can say it now,i will remember\"),\n",
    "    (\"user\",\"my name is rahul\")\n",
    "]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "746217c3-11a0-4b4c-9a2c-73c4a27febd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    {\"user\": \"multiply 5 with 10\",\"ai\":\"Multiplication: 50\"},\n",
    "    {\"user\": \"add 34 with 64\", \"ai\":\"Addition: 98\"}\n",
    "]\n",
    "\n",
    "fewshot_message_template = ChatPromptTemplate([\n",
    "    (\"user\",\"{user}\"),\n",
    "    (\"ai\",\"{ai}\")\n",
    "])\n",
    "\n",
    "fewshot_temp = FewShotChatMessagePromptTemplate(example_prompt=fewshot_message_template,\n",
    "                                               examples=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "365ded7c-a51e-48ec-91ae-849e510f52e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='multiply 5 with 10', additional_kwargs={}, response_metadata={}), AIMessage(content='Multiplication: 50', additional_kwargs={}, response_metadata={}), HumanMessage(content='add 34 with 64', additional_kwargs={}, response_metadata={}), AIMessage(content='Addition: 98', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshot_temp.invoke({}) # sometimes it is just few-shot we dont need to pass other values.so we used empty {} in invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9e803311-0ea6-4a41-b968-aaf724bdc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate([\n",
    "    (\"system\",\"You are a helpful ai assistant\"),\n",
    "    fewshot_temp,\n",
    "    (\"user\",\"add 6 with 6\")\n",
    "])\n",
    "\n",
    "special_chainer = final_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6ca3c017-36b6-487f-a0c2-f428c3da8a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nAlright, so I\\'ve got this math problem here: \"add 6 with 6.\" Okay, that seems pretty straightforward, but since I\\'m just starting out with this whole math thing, I want to make sure I understand it completely. Maybe I should break it down step by step to really grasp what\\'s going on.\\n\\nFirst off, what does it mean to \"add\" two numbers? I think adding is like combining two quantities together to get a total. So if I have 6 apples and someone gives me another 6 apples, how many apples do I have in total? That\\'s basically what addition is, right? It\\'s putting things together to find out how much you have altogether.\\n\\nOkay, so in this case, I have the number 6, and I need to add another 6 to it. Let me write that down to visualize it better: 6 + 6. Now, I need to figure out what 6 plus 6 equals. I remember from school that addition is one of the basic operations, and it\\'s pretty fundamental. But I want to make sure I\\'m not just memorizing the answer without understanding the process.\\n\\nMaybe I can use my fingers to help me out. If I hold up one hand with all five fingers extended and then hold up another hand with all five fingers extended, that\\'s 10 fingers in total. But wait, I\\'m supposed to add 6 and 6, not 5 and 5. Hmm, maybe I can adjust that. If I have six fingers on each hand, that would be 12 fingers in total. But I don\\'t have six fingers on each hand, so maybe using fingers isn\\'t the best method here.\\n\\nPerhaps I can use objects around me to represent the numbers. Let\\'s say I have six coins in one pile and another six coins in another pile. If I combine them, how many coins do I have? I can count them one by one to make sure. Starting with the first pile: 1, 2, 3, 4, 5, 6. Then the second pile: 1, 2, 3, 4, 5, 6. Adding them together, I count: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. So that\\'s 12 coins in total. Okay, that makes sense.\\n\\nBut I feel like I should understand this more abstractly, without having to count each time. Maybe I can think about it in terms of groups. If I have one group of 6 and another group of 6, how many groups do I have? Well, I have two groups, each with 6 items. So, the total number of items is the number of groups multiplied by the number of items in each group. Wait, that\\'s multiplication, not addition. Hmm, maybe that\\'s a different concept.\\n\\nNo, wait, in this case, I\\'m just combining the items from both groups into one big group. So it\\'s not about multiplying, it\\'s still about adding. So, 6 plus 6 is just combining the two groups into one, which gives me 12.\\n\\nI wonder if there\\'s a pattern or a shortcut for adding the same number twice. Like, 6 plus 6, 7 plus 7, 8 plus 8, and so on. It seems like doubling a number is the same as adding it to itself. So, 6 plus 6 is the same as 2 times 6, which is 12. That might be a quicker way to do it without having to count each time.\\n\\nBut is that always true? Let me test it with another number to see if this pattern holds. For example, 5 plus 5. If I add 5 and 5, I get 10. And 2 times 5 is also 10. Okay, that works. How about 4 plus 4? That\\'s 8, and 2 times 4 is also 8. Seems consistent. So, adding a number to itself is the same as multiplying it by 2. That\\'s a useful shortcut.\\n\\nBut what if the numbers are different? Like 3 plus 5? Is there a similar shortcut? I don\\'t think so. It seems like the doubling method only works when both numbers are the same. So, for adding different numbers, I might need to stick to regular addition methods or use other techniques like counting on or using a number line.\\n\\nGoing back to 6 plus 6, I think I\\'ve got a good understanding now. Whether I count them out, use objects to represent the numbers, or use the doubling method, the result is consistently 12. It\\'s reassuring to see that different methods lead to the same answer. This consistency is probably why addition is a fundamental operation‚Äîit provides a reliable way to combine quantities.\\n\\nI also recall that addition is commutative, which means the order of the numbers doesn\\'t affect the sum. So, 6 plus 6 is the same as 6 plus 6, which is obvious in this case, but it\\'s good to remember for when the numbers are different. For example, 3 plus 5 is the same as 5 plus 3, both equaling 8.\\n\\nAnother thing I\\'m thinking about is the concept of zero in addition. If I add zero to any number, the result is the number itself. So, 6 plus 0 is 6. This is important because it helps in understanding the identity element in addition. It\\'s like adding nothing, so the quantity remains unchanged.\\n\\nBut that\\'s probably beyond what I need to know for just adding 6 and 6. Still, it\\'s interesting to see how these basic concepts fit together in the broader scope of mathematics. Maybe understanding these underlying principles will help me grasp more complex math in the future.\\n\\nI also wonder how addition is used in real-life scenarios. For example, if I have 6 apples in a basket and someone gives me another 6 apples, how many apples do I have in total? It\\'s a practical application of addition. Similarly, if I have 6 dollars and earn another 6 dollars, I can add them to find out my total amount of money.\\n\\nIn summary, adding 6 and 6 involves combining two quantities of 6 to get a total of 12. This can be done by counting the items, using the doubling method, or understanding the commutative property of addition. It\\'s a fundamental skill that has practical applications in everyday life.\\n</think>\\n\\nThe result of adding 6 and 6 is 12.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1381, 'prompt_tokens': 44, 'total_tokens': 1425, 'completion_time': 5.021818182, 'prompt_time': 0.006877885, 'queue_time': 0.022370321999999998, 'total_time': 5.028696067}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_af2982c95d', 'finish_reason': 'stop', 'logprobs': None}, id='run-8993974a-6c5c-4e02-8b26-ba789b7bf993-0', usage_metadata={'input_tokens': 44, 'output_tokens': 1381, 'total_tokens': 1425})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chainer.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666a400-e2da-4cd7-9d2e-b0744b88d683",
   "metadata": {},
   "source": [
    "# Time to Recap: LangChain Message Types & Structured Inputs\n",
    "\n",
    "## üìå Message Types\n",
    "\n",
    "1. We have explored different message types in LangChain.\n",
    "2. Commonly used attributes in these message types are:\n",
    "   - `content`\n",
    "   - `name`\n",
    "   - `id`\n",
    "3. `AIMessage` has one more important attribute: `tool_calls` (used to call tools).\n",
    "4. `ToolMessage` has three additional important attributes:\n",
    "   - `tool_call_id`\n",
    "   - `artifact`\n",
    "   - `status`\n",
    "5. There is a concept called **Chunks** for all message types. These come into play when streaming LLM output.\n",
    "6. We can:\n",
    "   - Combine two message chunks to create another chunk.\n",
    "   - Add two or more `MessageTypes` to create a `ChatPromptTemplate`.\n",
    "\n",
    "## üèóÔ∏è Structured Inputs\n",
    "\n",
    "1. There are mainly two types of input to LLM:\n",
    "   - **Prompt** (string)\n",
    "   - **Chat prompt list** (messages)\n",
    "2. A prompt can be either a normal string or a docstring.\n",
    "3. Chat prompts can be of three types:\n",
    "   1. List of dictionaries (OpenAI style)\n",
    "   2. List of tuples\n",
    "   3. List of LangChain message types\n",
    "4. Just like models and tools, even a prompt is **invokable**.\n",
    "5. Since there are two types of inputs to LLM, at a high level, there are two types of templates:\n",
    "   - `PromptTemplate`\n",
    "   - `ChatPromptTemplate`\n",
    "6. `PromptTemplate` supports only strings, while `ChatPromptTemplate` supports both:\n",
    "   - List of dictionaries\n",
    "   - List of tuples\n",
    "7. Specific message type templates like `SystemMessagePromptTemplate`, `HumanMessagePromptTemplate`, etc., should be used in `ChatPromptTemplate` for LangChain message types.\n",
    "8. We can extend **few-shot examples** using respective `PromptTemplates`:\n",
    "   - For a **few-shot prompt template**, we need:\n",
    "     - `examples`\n",
    "     - `example_prompt` (which is a `PromptTemplate`)\n",
    "   - For a **few-shot chat prompt template**, we need:\n",
    "     - `examples`\n",
    "     - `example_prompt` (which is a `ChatPromptTemplate`)\n",
    "9. When we talk about **configurable prompts**, we can control:\n",
    "   - Specific parts in a message\n",
    "   - Adding multiple messages to `ChatPromptTemplate` as a placeholder using `MessagePlaceholder` (only works for chat prompt messages)\n",
    "10. With structured inputs, it is better to use **LCEL syntax** for better code. Otherwise, we have to use `invoke` for passing the prompt to the template and for the model too.\n",
    "\n",
    "üöÄ **practicing a lot to master these concepts!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cddff9-3b6f-4efe-932f-0144035a3aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
